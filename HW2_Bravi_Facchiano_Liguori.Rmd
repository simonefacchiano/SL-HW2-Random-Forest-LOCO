---
title: "HW2_Bravi_Facchiano_Liguori"
author: "simone facchiano"
date: "6/7/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packets

```{r}
library(readr)
library(ggplot2)
library(corrplot)
library(caret)
library(dplyr)

```

## Initialization

First, we imported the packages and data.

```{r pressure, echo=FALSE}
setwd("~/Desktop/Data Science/SL/Homework 2")

train <- read_csv("train_hw03.csv")
```


## Preprocessing

At this point we began with an initial analysis of the data. The first thing we noticed was the fact that different patients seemed to report measurements on different scales. Most of them all had values close to zero, while others (for example, patient number 2) showed extremely larger values. <br>

Since our ultimate goal was to construct a classifier capable of distinguishing between the two classes 'autism' and 'control,' we decided to standardize (by row) our dataset. This ensured that we had comparable data between them, and we could proceed with our work.

```{r}
# Problema: i valori (alcuni intorno allo 0, altri del tutto sballati)
hist(rowMeans(train[, -c(1:4)]), breaks = 20) # ad esempio, qui la seconda riga ha media -3, mentre tutte le altre sono intorno allo 0

# Creaiamo una copia del dataset, ma senza le prime 4 colonne
train_info <- train[, c(1:4)]
train_roi <- train[, -c(1:4)]

hist(rowMeans(train_roi), border = F)

# Scale
# 1)
train_roi_scale = data.frame(t(apply(train_roi, 1, scale)))
```

Standardization was only our first step toward building a dataset that would allow us to build a suitable classifier. <br>
Indeed, although a high number of variables is generally a good starting point for building a good model, we cannot forget the danger of overfitting that would compromise our results. <br>

We therefore opted for a dimensional reduction in our data. <br>
The most immediate idea was to use PCA, but we ran into a number of problems. Indeed, not only was it not easy to identify a simple 'cut-off' point from the graph of the cumulative variance of the principal components, but it was also not easy to give an interpretation of the variables we were using. And since we had to select the most important variables with specific techniques, this could be quite a problem. <br>

We decided to change our strategy. <br>
Since the rows of the dataset were in fact so many time-series concatenated one after the other, the choice fell on extracting some indices for each time-series, and to use them as new features for our model. <br>
Since the rows had been standardized, taking the mean and standard deviation would not have made sense: we therefore opted for more robust position indicators, such as quartiles, maximum and minimum. <br>
These indices were extracted for each historical series, and then put together to create a new reduced dataset.

```{r}
# 2)

q1_roi <- as.data.frame(sapply(seq(1, ncol(train_roi_scale), 115), function(start) {
  end <- start + 115 - 1
  apply(train_roi_scale[, start:end, drop = F], MARGIN = 1, FUN=quantile, probs=0.25)
}))

q2_roi <- as.data.frame(sapply(seq(1, ncol(train_roi_scale), 115), function(start) {
  end <- start + 115 - 1
  apply(train_roi_scale[, start:end, drop = F], MARGIN = 1, FUN=quantile, probs=0.5)
}))

q3_roi <- as.data.frame(sapply(seq(1, ncol(train_roi_scale), 115), function(start) {
  end <- start + 115 - 1
  apply(train_roi_scale[, start:end, drop = F], MARGIN = 1, FUN=quantile, probs=0.75)
}))

# sd_roi <- as.data.frame(sapply(seq(1, ncol(train_roi_scale), 115), function(start) {
#   end <- start + 115 - 1
#   apply(train_roi_scale[, start:end, drop = F], MARGIN = 1, FUN=sd)
# }))

# mean_roi <- as.data.frame(sapply(seq(1, ncol(train_roi_scale), 115), function(start) {
#   end <- start + 115 - 1
#   apply(train_roi_scale[, start:end, drop = F], MARGIN = 1, FUN=mean)
# }))

max_roi <- as.data.frame(sapply(seq(1, ncol(train_roi_scale), 115), function(start) {
  end <- start + 115 - 1
  apply(train_roi_scale[, start:end, drop = F], MARGIN = 1, FUN=max)
}))

min_roi <- as.data.frame(sapply(seq(1, ncol(train_roi_scale), 115), function(start) {
  end <- start + 115 - 1
  apply(train_roi_scale[, start:end, drop = F], MARGIN = 1, FUN=min)
}))



roi <- unique(
  sub(".*_", "", colnames(train_roi))
)

# Ora posso inserirle nell'altra tabella:
colnames(q1_roi) = paste0('q1_', roi) # molto meglio
colnames(q2_roi) = paste0('q2_', roi)
colnames(q3_roi) = paste0('q3_', roi)
#colnames(sd_roi) = paste0('sd_', roi)
#colnames(mean_roi) = paste0('mean_', roi)
colnames(max_roi) = paste0('max_', roi)
colnames(min_roi) = paste0('min_', roi)


prova = cbind(train_info, q1_roi, q2_roi, q3_roi, min_roi, max_roi)
prova = prova[, -1]

prova$sex = as.factor(ifelse(prova$sex == 'male', 1, 0))
prova$y = as.factor(ifelse(prova$y == 'autism', 1, 0))

```

## The Model: Random Forest for Classification

At this point we were ready to build our model. The choice that seemed most suitable to us was to use a Random Forest. Moreover, despite the reduction in size of the dataset, the number of variables still appeared quite large. Therefore, we chose to use again the caret library, which integrates within it a method specifically referred to the Random Forest.

```{r}
control <- trainControl(method='cv', 
                        number = 5)

metric <- "Accuracy"
set.seed(123)
mtry <- round(sqrt(ncol(prova) - 1)) # optimal number of variables for the bootstrap

tunegrid <- expand.grid(.mtry=mtry)

rf_default <- train(y ~. , 
                    data = prova, 
                    method = 'rf', 
                    metric = 'Accuracy', 
                    tuneGrid = tunegrid,
                    trControl = control)
print(rf_default)
```

One of the reasons the Random Forest seemed like a good idea is that we can easily extract a measure of Variable Importance from the model. Once this quantity was calculated for all variables we plotted the result. It seemed reasonable to us to focus on the first 50 variables, which offered the right trade-off between model simplicity and good performance.

```{r}
# VARIABLE IMPORTANCE
best_mtry <- as.numeric(rf_default$bestTune)
baseline_accuracy <- rf_default$results$Accuracy[rf_default$results$mtry == best_mtry]

var_imp = data.frame(varImp(rf_default, scale = F)$importance)
var_imp$variable = rownames(var_imp)
var_imp <- data.frame(var_imp[order(var_imp$Overall, decreasing = TRUE), ])

plot(var_imp[, 1], type = 'l', lwd = 2.5, col = 'steelblue', ylab = 'Variable Importance')
grid()
abline(v = 50, col = 'red', lwd = 1.7, lty = 2)

top_variables = var_imp[1:50, 2]
```

Having reached this point, we built a new model using the 50 variables just extracted. The only hyperparameter we tried to vary in our tunegrid concerns the number of variables sampled during the costruction of the trees. As expected, the parameter value that provided the best performance was the one closest to sqrt(ncol).

```{r}
# Nuovo modello con 50 variabili ------------------------------------------

prova_10_corr <- prova[, c(top_variables)]
prova_10_corr$y <- prova$y

control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3,
                        search = 'random')

tunegrid <- expand.grid(.mtry=c(6, 7, 8, 9))
rf_default10 <- train(y ~. , 
                      data = prova_10_corr, 
                      method = 'rf', 
                      metric = 'Accuracy', 
                      tuneGrid = tunegrid,
                      trControl = control)
print(rf_default10)
```

Having reached this point, we can consider our model finished. However, we are curious to find out whether the Variable Importance we obtained from the random forest coincides with the scores we can obtain from the LOCO procedure
[ SPIEGA LA LOCO ]

## LOCO: an alternative view on Variable Importance

```{r}
prova_10 <- prova[, c(top_variables[1:13], 'y')] # <-- OUR TOP VARIABLES + FLOP variables

# STEP 0: check for highly correlated variables
corrplot(cor(prova_10[, -14]))


high_cor_pairs <- which(abs(cor(prova_10[, -14])) > 0.6 & cor(prova_10[, -14]) < 0.99, arr.ind = TRUE)
discard_variables <- c()
for (i in 1:nrow(high_cor_pairs)) {
  variable1 <- colnames(cor(prova_10[, -14]))[high_cor_pairs[i, 1]]
  variable2 <- colnames(cor(prova_10[, -14]))[high_cor_pairs[i, 2]]
  
  ifelse(var_imp[variable1, 'Overall'] > var_imp[variable2, 'Overall'],
         discard_variables <- c(discard_variables, variable2),
         discard_variables <- c(discard_variables, variable1))
}

discard_variables <- unique(discard_variables)

LOCO_variables <- prova_10 %>%
  select(-one_of(discard_variables))

# STEP 1: partition in train e test
idx <- createDataPartition(LOCO_variables$y, p = 0.8, list = FALSE)
prova_10_train <- LOCO_variables[-idx,] # 80%
prova_10_test <- LOCO_variables[idx,] # 20%

# STEP 2: Run algorithm to compute estimate fˆ on first part D1
mtry <- round(sqrt(ncol(prova_10_train) - 1)) # optimal number of columns
tunegrid <- expand.grid(.mtry=mtry)
control <- trainControl(method='cv', 
                        number = 5)

rf_default <- train(y ~. , 
                    data = prova_10_train, 
                    method = 'rf', 
                    metric = 'Accuracy', 
                    tuneGrid = tunegrid,
                    trControl = control)
# print(rf_default)
f_hat <- factor(predict(rf_default, newdata = prova_10_train))
y_true <- factor(prova_10_train$y)
F1 = confusionMatrix(f_hat, y_true, mode = "everything", positive="1")$byClass[7]


# STEP 3:
# Si usa il test per ottenere M campioni bootstrap, e per ogni campione si fa il predict e si calcola l'F1 score. Si confronta F1_j con F1 per ognuno dei M campioni, e alla fine si prende la mediana.
LOCO_scores <- list()
LOCO_punctual <- c()

for(j in 1:(ncol(LOCO_variables)-1)){
  rf_j <- train(y ~. ,
                data = prova_10_train[, -j],
                method = 'rf',
                metric = 'Accuracy',
                tuneGrid = tunegrid,
                trControl = control)
  
  vec <- c()
  
  for(m in 1:50){
    idx_boot <- sample(1:119, size = 119, replace = T)
    sample_boot <- prova_10_test[idx_boot, ]
    
    f_hat_j <- factor(predict(rf_j, newdata = sample_boot))
    y_true <- factor(sample_boot$y)
    
    F1_j = confusionMatrix(f_hat_j, y_true, mode = "everything", positive="1")$byClass[7]
    
    vec <- c(vec, F1_j)
  }
  
  LOCO_scores[[j]] <- c(mean(F1-vec) - 2*sd(F1-vec), mean(F1-vec) + 2*sd(F1-vec))
  LOCO_punctual <- c(LOCO_punctual, median(F1-vec))
  
}

LOCO_scores
LOCO_punctual
```

We can also plot the LOCO scores:

```{r}

df <- data.frame(do.call(rbind, LOCO_scores))
LOCO_punctual_df <- data.frame(x = 1:10, y = LOCO_punctual)

x = seq(1, 10, length.out = 10)
y = seq(0, 1, length.out=10)
brks <- seq(0, 1, 0.1)
ggplot(data = df,
       mapping = aes(x=x,y=y))+
  geom_errorbar(alpha=1.5, linetype=1, size=0.8,
                ymin=df$X1,
                ymax=df$X2,
                color="#a3c4dc")+ 
  geom_point(x = LOCO_punctual_df$x, y = LOCO_punctual_df$y, col = "#0e668b")+
  coord_flip()+
  scale_y_continuous(breaks = brks, labels = brks)+
  scale_x_continuous(breaks = seq(0, 10, 1), labels = seq(0, 10, 1))+
  labs(title="Loco confidence intervals",
       x = "Variable Number",
       y = "LOCO score")+ 
  theme(text = element_text(family = "serif"), 
        plot.title = element_text(hjust=0.5),
        #plot.background=element_rect(fill="white"),
        panel.background=element_rect(fill="#fafafa"),
        panel.grid.minor=element_blank(),
        panel.grid.major.y=element_blank(),
        panel.grid.major.x=element_line(),
        axis.ticks=element_blank())
```

## Predictions

A questo punto, siamo pronti per fare le nostre previsioni. Importiamo dunque il test set, facciamo dimensionality reduction ed estraiamo le 50 variabili più importanti.

```{r}
test <- read_csv("test_hw03.csv")
head(test)


test_info <- test[, c(1:3)]
test_roi <- test[, -c(1:3)]

hist(rowMeans(test_roi))

# Scale
# 1)
test_roi_scale = data.frame(t(apply(test_roi, 1, scale)))

# 2)

q1_roi_test <- as.data.frame(sapply(seq(1, ncol(test_roi_scale), 115), function(start) {
  end <- start + 115 - 1
  apply(test_roi_scale[, start:end, drop = F], MARGIN = 1, FUN=quantile, probs=0.25)
}))

q2_roi_test <- as.data.frame(sapply(seq(1, ncol(test_roi_scale), 115), function(start) {
  end <- start + 115 - 1
  apply(test_roi_scale[, start:end, drop = F], MARGIN = 1, FUN=quantile, probs=0.5)
}))

q3_roi_test <- as.data.frame(sapply(seq(1, ncol(test_roi_scale), 115), function(start) {
  end <- start + 115 - 1
  apply(test_roi_scale[, start:end, drop = F], MARGIN = 1, FUN=quantile, probs=0.75)
}))

# sd_roi_test <- as.data.frame(sapply(seq(1, ncol(test_roi_scale), 115), function(start) {
#   end <- start + 115 - 1
#   apply(test_roi_scale[, start:end, drop = F], MARGIN = 1, FUN=sd)
# }))
# 
# mean_roi_test <- as.data.frame(sapply(seq(1, ncol(test_roi_scale), 115), function(start) {
#   end <- start + 115 - 1
#   apply(test_roi_scale[, start:end, drop = F], MARGIN = 1, FUN=mean)
# }))

# cor_roi <- as.data.frame(sapply(seq(1, ncol(test_roi_scale), 115), function(start) {
#   end <- start + 115 - 1
#   apply(test_roi_scale[, start:end, drop = F], MARGIN = 1, FUN=cor)
# }))

max_roi_test <- as.data.frame(sapply(seq(1, ncol(test_roi_scale), 115), function(start) {
  end <- start + 115 - 1
  apply(test_roi_scale[, start:end, drop = F], MARGIN = 1, FUN=max)
}))

min_roi_test <- as.data.frame(sapply(seq(1, ncol(test_roi_scale), 115), function(start) {
  end <- start + 115 - 1
  apply(test_roi_scale[, start:end, drop = F], MARGIN = 1, FUN=min)
}))


# Ora posso inserirle nell'altra tabella:
colnames(q1_roi_test) = paste0('q1_', roi) # molto meglio
colnames(q2_roi_test) = paste0('q2_', roi)
colnames(q3_roi_test) = paste0('q3_', roi)
colnames(max_roi_test) = paste0('max_', roi)
colnames(min_roi_test) = paste0('min_', roi)


prova_test = cbind(test_info, q1_roi_test, q2_roi_test, q3_roi_test, min_roi_test, max_roi_test)
prova_test = prova_test[, -1]

prova_test$sex = as.factor(ifelse(prova_test$sex == 'male', 1, 0))

colnames(prova_10_corr)[1:50] # --> sono le nostre variabili, senza la y
prova_10_test <- prova_test[, colnames(prova_10_corr)[1:50]]

```


Infine, procediamo con il predict del modello, e salviamo i risultati in un file .csv che caricheremo su Kaggle.

```{r}
id <- test$id
target = predict(rf_default10, newdata = prova_10_test)
target = ifelse(target == 1, 'autism', 'control')

preds_g11 = data.frame('id' = id, 'target' = target)
table(preds_g11$target)

write.csv(preds_g11, file = "preds_g11_13.csv", row.names = FALSE)
```